1. We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.
2. 	a. size of the hidden layers n^[l]
	b. learning rate a (alpha)
	c. number of iterations
	d. number of layers L in the neural network
3. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.
4. False
5. for(i in range(1, len(layer_dims))):
    parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i - 1])) * 0.01
    parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01
6. The number of layers L is 4. The number of hidden layers is 3.
7. True
8. True
9. 	a. W^[1] will have shape (4, 4)
	b. b^[1] will have shape (4, 1)
	c. W^[2] will have shape (3, 4)
	d. b^[2] will have shape (3, 1)
	e. b^[3] will have shape (1, 1)
	f. W^[3] will have shape (1, 3)
10. W^[l] has shape (n^[l],n^[l-1])